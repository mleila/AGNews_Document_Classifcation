{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from news_classifier.data import assign_rows_to_split\n",
    "from news_classifier.constants import DATASET, TRAIN, TEST, VALID, LABEL_COL\n",
    "\n",
    "# notebook wide constants\n",
    "HOME_DIR = Path('..')\n",
    "DATA_DIR = HOME_DIR / 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                              title\n",
       "0  Business  Wall St. Bears Claw Back Into the Black (Reuters)\n",
       "1  Business  Carlyle Looks Toward Commercial Aerospace (Reu...\n",
       "2  Business    Oil and Economy Cloud Stocks' Outlook (Reuters)\n",
       "3  Business  Iraq Halts Oil Exports from Main Southern Pipe...\n",
       "4  Business  Oil prices soar to all-time record, posing new..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "df = pd.read_csv(DATA_DIR / 'news.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size 252_000, Valid size 54_000, Test size 54_000\n"
     ]
    }
   ],
   "source": [
    "# use the assign_rows_to_split to split rows into either train,test or valid while stratifying wrt the categories\n",
    "splitted_df = assign_rows_to_split(df)\n",
    "\n",
    "train_rows = splitted_df.query(f'{DATASET}==\"{TRAIN}\"')\n",
    "valid_rows = splitted_df.query(f'{DATASET}==\"{VALID}\"')\n",
    "test_rows = splitted_df.query(f'{DATASET}==\"{TEST}\"')\n",
    "\n",
    "print(f'Train size {train_rows.size:_}, Valid size {valid_rows.size:_}, Test size {test_rows.size:_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "World       21000\n",
       "Sports      21000\n",
       "Business    21000\n",
       "Sci/Tech    21000\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rows[LABEL_COL].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "World       4500\n",
       "Business    4500\n",
       "Sci/Tech    4500\n",
       "Sports      4500\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_rows[LABEL_COL].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write splitted file to disk\n",
    "fp = DATA_DIR / 'news_splitted.csv'\n",
    "splitted_df.to_csv(fp, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from news_classifier.data import Dataset, generate_batches\n",
    "from news_classifier.utils import save_json\n",
    "\n",
    "data_path = DATA_DIR / 'news_splitted.csv'\n",
    "splitted_df = pd.read_csv(data_path)\n",
    "dataset = Dataset.from_dataframe(splitted_df.iloc[:25_000])\n",
    "\n",
    "vectorizer = dataset.vectorizer\n",
    "vserialized_vectorizer = vectorizer.to_serializable()\n",
    "\n",
    "vectorizer_path = HOME_DIR / 'language/vectorizer.json'\n",
    "save_json(vserialized_vectorizer, vectorizer_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Saving checkpoint\n",
      "Completed Epoch 0 with average loss of 1.04\n",
      "=> Saving checkpoint\n",
      "Completed Epoch 1 with average loss of 0.84\n",
      "=> Saving checkpoint\n",
      "Completed Epoch 2 with average loss of 0.80\n",
      "=> Saving checkpoint\n",
      "Completed Epoch 3 with average loss of 0.78\n",
      "=> Saving checkpoint\n",
      "Completed Epoch 4 with average loss of 0.77\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from news_classifier.data import Dataset, generate_batches\n",
    "from news_classifier.models import DNN_Classifier\n",
    "from news_classifier.training import Trainer\n",
    "\n",
    "# dataset\n",
    "data_path = DATA_DIR / 'news_splitted.csv'\n",
    "splitted_df = pd.read_csv(data_path)\n",
    "dataset = Dataset.from_dataframe(splitted_df.iloc[:25_000])\n",
    "\n",
    "# model dimensions\n",
    "input_dim = len(dataset.vectorizer.headlines_vocab)\n",
    "nb_categories = len(dataset.vectorizer.labels_vocab)\n",
    "\n",
    "# create model\n",
    "model = DNN_Classifier(input_dim=input_dim, nb_categories=nb_categories)\n",
    "model_dir = HOME_DIR / 'models/DNN_Classifier'\n",
    "\n",
    "# loss function\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "# device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# create trainer object\n",
    "trainer = Trainer(\n",
    "    data_loader=generate_batches, \n",
    "    optimizer=optimizer, \n",
    "    model=model, \n",
    "    model_dir=model_dir, \n",
    "    loss_func=loss_func, \n",
    "    device=device\n",
    ")\n",
    "\n",
    "# training params\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "trainer.run(nb_epochs=epochs, dataset=dataset, batch_size=batch_size, checkpoint=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Predicted Category: Sports\n"
     ]
    }
   ],
   "source": [
    "from news_classifier.utils import load_json, get_latest_model_checkpoint, load_checkpoint\n",
    "from news_classifier.language import Vectorizer\n",
    "from news_classifier.models import DNN_Classifier\n",
    "\n",
    "# headline\n",
    "headline = 'the team won a grant'\n",
    "\n",
    "# vectorizer\n",
    "headline_vocab_path = HOME_DIR / 'language/vectorizer.json'\n",
    "headlines_vectorizer = load_json(headline_vocab_path)\n",
    "vectorizer = Vectorizer.from_serializable(headlines_vectorizer)\n",
    "vectorized = vectorizer.vectorize_headline(headline)\n",
    "\n",
    "# embed in a batch\n",
    "infer_batch = torch.tensor(vectorized).unsqueeze(0)\n",
    "\n",
    "# select model\n",
    "input_dim = len(vectorizer.headlines_vocab)\n",
    "nb_categories = len(vectorizer.labels_vocab)\n",
    "model = DNN_Classifier(input_dim=input_dim, nb_categories=nb_categories)\n",
    "\n",
    "# load latest checkpoints\n",
    "MODEL_DIR = HOME_DIR / 'models/DNN_Classifier'\n",
    "latest_checkpoint = get_latest_model_checkpoint(MODEL_DIR)\n",
    "checkpoint_state = torch.load(latest_checkpoint, map_location=torch.device('cpu'))\n",
    "load_checkpoint(checkpoint_state, model)\n",
    "\n",
    "# inference\n",
    "prediction = model(infer_batch)\n",
    "\n",
    "# pick most liklely index\n",
    "index = torch.argmax(prediction).item()\n",
    "label_string = vectorizer.labels_vocab.lookup_index(index)\n",
    "\n",
    "print(f' Predicted Category: {label_string}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pDL] *",
   "language": "python",
   "name": "conda-env-pDL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
