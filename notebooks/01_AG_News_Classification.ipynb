{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from news_classifier.data import assign_rows_to_split\n",
    "from news_classifier.constants import DATASET, TRAIN, TEST, VALID, LABEL_COL\n",
    "# notebook wide constants\n",
    "DATA_DIR = Path('../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                              title\n",
       "0  Business  Wall St. Bears Claw Back Into the Black (Reuters)\n",
       "1  Business  Carlyle Looks Toward Commercial Aerospace (Reu...\n",
       "2  Business    Oil and Economy Cloud Stocks' Outlook (Reuters)\n",
       "3  Business  Iraq Halts Oil Exports from Main Southern Pipe...\n",
       "4  Business  Oil prices soar to all-time record, posing new..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data\n",
    "df = pd.read_csv(DATA_DIR / 'news.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size 252_000, Valid size 54_000, Test size 54_000\n"
     ]
    }
   ],
   "source": [
    "# use the assign_rows_to_split to split rows into either train,test or valid while stratifying wrt the categories\n",
    "splitted_df = assign_rows_to_split(df)\n",
    "\n",
    "train_rows = splitted_df.query(f'{DATASET}==\"{TRAIN}\"')\n",
    "valid_rows = splitted_df.query(f'{DATASET}==\"{VALID}\"')\n",
    "test_rows = splitted_df.query(f'{DATASET}==\"{TEST}\"')\n",
    "\n",
    "print(f'Train size {train_rows.size:_}, Valid size {valid_rows.size:_}, Test size {test_rows.size:_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "World       21000\n",
       "Sports      21000\n",
       "Sci/Tech    21000\n",
       "Business    21000\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rows[LABEL_COL].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sci/Tech    4500\n",
       "World       4500\n",
       "Sports      4500\n",
       "Business    4500\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_rows[LABEL_COL].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write splitted file to disk\n",
    "fp = DATA_DIR / 'news_splitted.csv'\n",
    "splitted_df.to_csv(fp, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "from news_classifier.data import Dataset, generate_batches\n",
    "\n",
    "dataset = Dataset.from_dataframe(splitted_df.iloc[:25_000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed Epoch 0 with average loss of 1.04\n",
      "Completed Epoch 1 with average loss of 0.85\n",
      "Completed Epoch 2 with average loss of 0.80\n",
      "Completed Epoch 3 with average loss of 0.78\n",
      "Completed Epoch 4 with average loss of 0.78\n"
     ]
    }
   ],
   "source": [
    "from news_classifier.models import DNN_Classifier\n",
    "\n",
    "# training params\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "# model dimensions\n",
    "input_dim = len(dataset.vectorizer.headlines_vocab)\n",
    "nb_categories = len(dataset.vectorizer.labels_vocab)\n",
    "\n",
    "# create model\n",
    "model = DNN_Classifier(input_dim=input_dim, nb_categories=nb_categories)\n",
    "\n",
    "# loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# optimizer\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    losses = []\n",
    "    for batch_gen in generate_batches(dataset, batch_size=batch_size):\n",
    "        x_in, y_true = batch_gen['x'], batch_gen['y']\n",
    "\n",
    "        # clear gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # x_in should be  [batch_size, nb_features] \n",
    "        # y_pred should be [batch_size, out_features]\n",
    "        y_pred = model(x_in)\n",
    "\n",
    "        loss = criterion(y_pred, y_true)\n",
    "        loss_batch = loss.item()\n",
    "        losses.append(loss_batch)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "    avg_loss = sum(losses)/len(losses)\n",
    "    print(f'Completed Epoch {epoch} with average loss of {avg_loss:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sci/Tech'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# headline\n",
    "headline = 'the team won a grant'\n",
    "vectorized = dataset.vectorizer.vectorize_headline(headline)\n",
    "\n",
    "# embed in a batch\n",
    "infer_batch = torch.tensor(vectorized).unsqueeze(0)\n",
    "prediction = model(infer_batch)\n",
    "\n",
    "# pick most liklely index\n",
    "index = torch.argmax(prediction).item()\n",
    "dataset.vectorizer.labels_vocab.lookup_index(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Business', 'Sci/Tech', 'Sports', 'World'], dtype=object)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.category.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pDL] *",
   "language": "python",
   "name": "conda-env-pDL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
